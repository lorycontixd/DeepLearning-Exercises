{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "862f27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "from enum import Enum\n",
    "from typing import Union\n",
    "\n",
    "# IPython\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#ML\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "84c7a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Math:\n",
    "    @staticmethod\n",
    "    def distance(x1, x2, y1, y2):\n",
    "        return np.sqrt( np.power( (x2-x1), 2.) + np.power( (y2-y1), 2.) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e838c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionType(Enum):\n",
    "    MOVEMENT = 0\n",
    "    ACTION = 1\n",
    "\n",
    "@dataclass\n",
    "class Action:\n",
    "    type: ActionType\n",
    "    value: Union[int, str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "84af1772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "\n",
    "@dataclass\n",
    "class QSettings():\n",
    "    epsilon: float\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.99\n",
    "    discount: float\n",
    "    learning_rate: float\n",
    "     \n",
    "    \n",
    "@dataclass\n",
    "class Timer:\n",
    "    start: float\n",
    "    end: float\n",
    "    timer: float\n",
    "        \n",
    "class RunInfo:\n",
    "    def __init__(self, nepisodes, ntimes):\n",
    "        self.timerstart = None\n",
    "        self.timerend = None\n",
    "        self.timertime = None\n",
    "        \n",
    "        self.scores = {str(i):[] for i in range(nepisodes)}\n",
    "    \n",
    "    def start_timer(self):\n",
    "        self.timerstart = time.time()\n",
    "    \n",
    "    def end_timer(self):\n",
    "        if self.timerstart is not None:\n",
    "            self.timerend = time.time()\n",
    "            self.timertime = self.timerend - self.timerend\n",
    "    \n",
    "    def get_timer(self):\n",
    "        if self.timertime is not None:\n",
    "            return self.timerend - self.timerstart\n",
    "        else:\n",
    "            raise ValueError(\"No timer is saved\")\n",
    "    \n",
    "    def reset_timer(self):\n",
    "        self.timerstart = None\n",
    "        self.timerend = None\n",
    "        self.timertime = None\n",
    "    \n",
    "    def add_score(self, episode, time, score):\n",
    "        self.scores[str(episode)].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d22106bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character:\n",
    "    def __init__(self, x, y, health: float, attdmg: float):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.startx = x\n",
    "        self.starty = y\n",
    "        self.health = health\n",
    "        self.attdmg = attdmg\n",
    "\n",
    "class Agent(Character):\n",
    "    def __init__(self, env, x, y, qsettings: QSettings):\n",
    "        super().__init__(x,y, 300, 20)\n",
    "        self.env = env\n",
    "        self.x, self.y = self._set_valid_start(self.x, self.y)\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.nstates = 2 # vertical movement (rows), horizontal movement (cols)\n",
    "        self.qsettings = qsettings\n",
    "        \n",
    "        self.memory = deque(maxlen=50000)\n",
    "        self.qtable = np.zeros((self.env.nrows, self.env.ncols, len(self.actions))) # nrows x ncols x nactions\n",
    "        \n",
    "        self.model = self.__build_model__()\n",
    "        self.target_model = self.__build_model__()\n",
    "        self.update_target_model()\n",
    "        \n",
    "    def _get_random_valid_pos(self):\n",
    "        x = -1\n",
    "        y = -1\n",
    "        while (self.env.is_end(x,y) or self.env.is_enemy(x,y)) or x == -1 or y == -1:\n",
    "            x = np.random.randint(0, self.env.nrows)\n",
    "            y = np.random.randint(0, self.env.ncols)\n",
    "        return x, y\n",
    "        \n",
    "    def _set_valid_start(self, x, y):\n",
    "        if self.env.is_end(x,y) or self.env.is_enemy(x,y):\n",
    "            return self._get_random_valid_pos()\n",
    "        else:\n",
    "            return x, y\n",
    "        \n",
    "    def _check_valid_start(self, x, y):\n",
    "        return not (self.env.is_end(x,y) or self.env.is_enemy(x,y))\n",
    "    \n",
    "    def __build_model__(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(50, input_dim=self.nstates, activation='relu'))\n",
    "        model.add(layers.Dense(50, activation='relu'))\n",
    "        model.add(layers.Dense(len(self.actions), activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=optimizers.Adam(lr=self.qsettings.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def set_starting_pos(self, *args):\n",
    "        if len(args) == 1:\n",
    "            # (x,y) as tuple\n",
    "            self.startx, self.starty = args[0]\n",
    "            if not self._check_valid_start(args[0][0], args[0][1]):\n",
    "                raise ValueError(\"Agent starting position is invalid\")\n",
    "        elif len(args) == 2:\n",
    "            # x,y as split params\n",
    "            if not self._check_valid_start(args[0], args[1]):\n",
    "                raise ValueError(\"Agent starting position is invalid\")\n",
    "            self.startx = args[0]\n",
    "            self.starty = args[1]\n",
    "        else:\n",
    "            raise ValueError(\"[SetStartingPos] Invalid number of arguments passed\")\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def get_next_action(self, state):\n",
    "        \"\"\"Next action is decided based on the epsilon-greedy algorithm.\n",
    "        If a random number is smaller than a value epsilon, the max value from the queue table is selected.\n",
    "        Otherwise, if the random number is larger, pick a random action\n",
    "        \"\"\"\n",
    "        if np.random.random() <= self.qsettings.epsilon:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            act_values = self.model.predict(state) # Main difference between vanilla q-learning and DQL\n",
    "            return np.argmax(act_values[0])\n",
    "            \n",
    "    \n",
    "    def get_next_location(self,current_row_index, current_column_index, action_index: int):\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.env.ncols - 1:\n",
    "            new_column_index += 1\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.env.nrows - 1:\n",
    "            new_row_index += 1\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        return new_row_index, new_column_index\n",
    "    \n",
    "    def reset(self, randomizepos):\n",
    "        if randomizepos:\n",
    "            self.startx, self.starty = self._get_random_valid_pos()\n",
    "            self.x = self.startx\n",
    "            self.y = self.starty\n",
    "        else:\n",
    "            self.set_starting_pos(self.startx, self.starty)\n",
    "        return self.build_state(self.x, self.y)\n",
    "        \n",
    "    def build_state(self, *args):\n",
    "        if len(args)==1:\n",
    "            # state\n",
    "            state = args[0]\n",
    "            return np.array([state,]) \n",
    "        elif len(args)==2:\n",
    "            # x, y\n",
    "            x = args[0]\n",
    "            y = args[1]\n",
    "            return np.array([[x, y],])  \n",
    "        else:\n",
    "            raise ValueError(\"[BuildState] Invalid number of arguments passed\")\n",
    "    \n",
    "    def step(self, actionindex):\n",
    "        # Take the step and calculate new state\n",
    "        self.x, self.y = self.get_next_location(self.x, self.y, actionindex)\n",
    "        newstate = self.build_state(self.x, self.y)\n",
    "        # Calculate reward\n",
    "        reward = self.env.rewards[self.x][self.y]\n",
    "        done = self.env.is_end(self.x, self.y) or self.env.is_enemy(self.x, self.y)\n",
    "        return newstate, reward, done\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.qsettings.discount * np.amax(t)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.qsettings.epsilon > self.qsettings.epsilon_min:\n",
    "            self.qsettings.epsilon *= self.qsettings.epsilon_decay\n",
    "    \n",
    "    def trainDNN(self, epochs: int, epoch_max_time: int, batch_size: int, randomizepos: bool = True, verbose: bool = False):\n",
    "        self.runinfo = RunInfo(epochs, epoch_max_time)\n",
    "        self.runinfo.start_timer()\n",
    "        for epoch in range(epochs):\n",
    "            state = self.reset(randomizepos)\n",
    "            done = False\n",
    "            for time in range(epoch_max_time):\n",
    "                print(f\"Running time {time+1}/{epoch_max_time} for epoch {epoch+1}/{epochs}\", end='\\r', flush=True)\n",
    "                ### Choose an action using the epsilon-greedy algorithm\n",
    "                actionindex = self.get_next_action(state)\n",
    "                newstate, reward, done = self.step(actionindex)\n",
    "                # Store transition ( s,a,r,s',done )\n",
    "                self.memorize(state, actionindex, reward, newstate, done)\n",
    "                state = newstate\n",
    "                if done:\n",
    "                    self.update_target_model()\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                          .format(epoch+1, epochs, time+1, self.qsettings.epsilon))\n",
    "                    break\n",
    "                else:\n",
    "                    if len(self.memory) > batch_size: # If enough experiences in memory -> replay\n",
    "                        self.replay(batch_size)\n",
    "                self.runinfo.add_score(epoch, time, self.qsettings.epsilon)\n",
    "        self.runinfo.end_timer()\n",
    "        traintime = self.runinfo.get_timer()\n",
    "        self.trained = True\n",
    "        print(f\"Train completed in {traintime} seconds\")\n",
    "        \n",
    "    \n",
    "    def run(self, startingpos: tuple[int], maxiterations: int = 10000, verbose: bool = False):\n",
    "        self.set_starting_pos(startingpos)\n",
    "        if self.env.is_enemy(self.startx, self.starty) or self.env.is_end(self.startx, self.starty):\n",
    "            return []\n",
    "        else:\n",
    "            state = self.reset(False)\n",
    "            shortestpath = []\n",
    "            shortestpath.append((self.startx, self.starty))\n",
    "            #i: int = 0\n",
    "            while not (self.env.is_end(self.x, self.y) or self.env.is_enemy(self.x, self.y)):\n",
    "                actionindex = self.get_next_action(state)\n",
    "                self.x, self.y = self.get_next_location(self.x, self.y, actionindex)\n",
    "                shortestpath.append((self.x, self.y))\n",
    "                print(f\"({self.x},{self.y})\", end='\\r',flush=True)\n",
    "                #i+=1\n",
    "                #if (i >= maxiterations):\n",
    "                #    print(f\"Reached maximum iterations {maxiterations}. Current position: ({self.x},{self.y})\")\n",
    "                #    return []\n",
    "            self.ran = True\n",
    "            return shortestpath\n",
    "    \n",
    "\n",
    "    \n",
    "class Enemy(Character):\n",
    "    def __init__(self, x, y, radius = 3) -> None:\n",
    "        super().__init__(x,y)\n",
    "        self.radius = radius\n",
    "        \n",
    "    def inrange(self, x,y):\n",
    "        return Math.distance(x,self.x, y, self.y) <= self.radius\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9556fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self, nrows: int, ncols: int, starting_location: tuple, end_location: tuple, nenemies: int, ):\n",
    "        self.nrows = nrows\n",
    "        self.ncols = ncols\n",
    "        self.mapsize = (nrows, ncols)\n",
    "        self.end_location = end_location\n",
    "        self.endrow, self.endcol = self.end_location\n",
    "        self.nenemies = nenemies\n",
    "        \n",
    "        \n",
    "        self.enemies = []\n",
    "        self.rewards = np.full((self.nrows, self.ncols), -1.)\n",
    "        \n",
    "        self.basereward = -1\n",
    "        self.enemyreward = -300\n",
    "        self.endreward = 300\n",
    "        \n",
    "        self.__build_enemies__()\n",
    "        self.__build_rewards__()\n",
    "    \n",
    "        \n",
    "    def __build_enemies__(self):\n",
    "        if len(self.enemies) <= 0:\n",
    "            while len(self.enemies) < self.nenemies:\n",
    "                randx = np.random.randint(self.nrows)\n",
    "                randy = np.random.randint(self.ncols)\n",
    "                if (randx, randy) not in self.enemies:\n",
    "                    if (randx,randy) != self.end_location:\n",
    "                        self.enemies.append((randx, randy))\n",
    "                        \n",
    "    def __build_rewards__(self):\n",
    "        for enemy in self.enemies:\n",
    "            ob_x, ob_y = enemy\n",
    "            self.rewards[ob_x][ob_y] = self.enemyreward\n",
    "        self.rewards[self.end_location[0]][self.end_location[1]] = self.endreward\n",
    "    \n",
    "    \n",
    "    def is_enemy(self, x, y):\n",
    "        return self.rewards[x][x] == self.enemyreward\n",
    "    def is_end(self, x, y):\n",
    "        return self.rewards[x][y] == self.endreward\n",
    "    \n",
    "    def showmap(self):\n",
    "        if len(self.enemies) > 0:\n",
    "            fig, ax = plt.subplots(figsize=(9,9))\n",
    "            ax.set_title(\"Reward map\")\n",
    "            ax.set_xlabel(\"col\")\n",
    "            ax.set_ylabel(\"row\")\n",
    "            rewardmap = self.rewards\n",
    "            ax.imshow(rewardmap)\n",
    "            plt.show()      \n",
    "    \n",
    "    def showpath(self, startpos: tuple, shortestpath: list):\n",
    "        if len(shortestpath) > 0:\n",
    "            newmap = self.rewards\n",
    "            newmap[startpos[0]][startpos[1]] = -50\n",
    "            for pos in shortestpath:\n",
    "                r, c = pos\n",
    "                newmap[r][c] = 100\n",
    "            fig, ax = plt.subplots(figsize=(9,9))\n",
    "            ax.set_title(\"Reward map\")\n",
    "            ax.set_xlabel(\"col\")\n",
    "            ax.set_ylabel(\"row\")\n",
    "            ax.imshow(newmap)\n",
    "            plt.show()\n",
    "    \n",
    "    def save(self, name): \n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f6855dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Environment(\n",
    "    30,30,\n",
    "    (1,1),\n",
    "    (28,28),\n",
    "    100\n",
    ")\n",
    "\n",
    "agentqsettings = QSettings(epsilon=0.9, discount=0.4, learning_rate=0.001)\n",
    "a = Agent(e, 1, 1, agentqsettings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19552e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "e.showmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eb941ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3/20, score: 33, e: 0.40\n",
      "episode: 4/20, score: 2, e: 0.40\n",
      "episode: 5/20, score: 1, e: 0.40\n",
      "episode: 6/20, score: 1, e: 0.40\n",
      "episode: 7/20, score: 1, e: 0.40\n",
      "episode: 8/20, score: 1, e: 0.40\n",
      "episode: 9/20, score: 20, e: 0.33\n",
      "episode: 10/20, score: 1, e: 0.33\n",
      "episode: 11/20, score: 1, e: 0.33\n",
      "episode: 16/20, score: 11, e: 0.06\n",
      "episode: 17/20, score: 1, e: 0.06\n",
      "episode: 18/20, score: 1, e: 0.06\n",
      "Train completed in 1833.5265560150146 seconds\n"
     ]
    }
   ],
   "source": [
    "a.trainDNN(20, 40, 32, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f58535b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reset() missing 1 required positional argument: 'randomizepos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-27f2eb0968dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstartpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshortestpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-57674197c017>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, startingpos, maxiterations, verbose)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0mshortestpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mshortestpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: reset() missing 1 required positional argument: 'randomizepos'"
     ]
    }
   ],
   "source": [
    "startpos = (1,1)\n",
    "shortestpath = a.run(startpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shortestpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.showpath(startpos, shortestpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a5c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(shortestpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb8951e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
