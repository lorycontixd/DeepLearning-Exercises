{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "862f27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "from enum import Enum\n",
    "from typing import Union\n",
    "\n",
    "# IPython\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#ML\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "84c7a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Math:\n",
    "    @staticmethod\n",
    "    def distance(x1, x2, y1, y2):\n",
    "        return np.sqrt( np.power( (x2-x1), 2.) + np.power( (y2-y1), 2.) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e838c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionType(Enum):\n",
    "    MOVEMENT = 0\n",
    "    ACTION = 1\n",
    "\n",
    "@dataclass\n",
    "class Action:\n",
    "    type: ActionType\n",
    "    value: Union[int, str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "84af1772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "\n",
    "@dataclass\n",
    "class QSettings():\n",
    "    epsilon: float\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.99\n",
    "    discount: float\n",
    "    learning_rate: float\n",
    "     \n",
    "    \n",
    "@dataclass\n",
    "class Timer:\n",
    "    start: float\n",
    "    end: float\n",
    "    timer: float\n",
    "        \n",
    "class RunInfo:\n",
    "    def __init__(self, nepisodes, ntimes):\n",
    "        self.timerstart = None\n",
    "        self.timerend = None\n",
    "        self.timertime = None\n",
    "        \n",
    "        self.scores = {str(i):[] for i in range(nepisodes)}\n",
    "    \n",
    "    def start_timer(self):\n",
    "        self.timerstart = time.time()\n",
    "    \n",
    "    def end_timer(self):\n",
    "        if self.timerstart is not None:\n",
    "            self.timerend = time.time()\n",
    "            self.timertime = self.timerend - self.timerend\n",
    "    \n",
    "    def get_timer(self):\n",
    "        if self.timertime is not None:\n",
    "            return self.timerend - self.timerstart\n",
    "        else:\n",
    "            raise ValueError(\"No timer is saved\")\n",
    "    \n",
    "    def reset_timer(self):\n",
    "        self.timerstart = None\n",
    "        self.timerend = None\n",
    "        self.timertime = None\n",
    "    \n",
    "    def add_score(self, episode, time, score):\n",
    "        self.scores[str(episode)].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d22106bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character:\n",
    "    def __init__(self, x, y, health: float, attdmg: float):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.startx = x\n",
    "        self.starty = y\n",
    "        self.health = health\n",
    "        self.attdmg = attdmg\n",
    "\n",
    "class Agent(Character):\n",
    "    def __init__(self, env, x, y, qsettings: QSettings, useDNN = True):\n",
    "        super().__init__(x,y, 300, 20)\n",
    "        self.env = env\n",
    "        self.x, self.y = self._set_valid_start(self.x, self.y)\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.nstates = 2 # vertical movement (rows), horizontal movement (cols)\n",
    "        self.qsettings = qsettings\n",
    "        \n",
    "        self.memory = deque(maxlen=50000)\n",
    "        self.qtable = np.zeros((self.env.nrows, self.env.ncols, len(self.actions))) # nrows x ncols x nactions\n",
    "        \n",
    "        self.model = self.__build_model__()\n",
    "        self.target_model = self.__build_model__()\n",
    "        self.update_target_model()\n",
    "        \n",
    "        self.useDNN = useDNN\n",
    "        \n",
    "    def _get_random_valid_pos(self):\n",
    "        x = -1\n",
    "        y = -1\n",
    "        while (self.env.is_end(x,y) or self.env.is_enemy(x,y)) or x == -1 or y == -1:\n",
    "            x = np.random.randint(0, self.env.nrows)\n",
    "            y = np.random.randint(0, self.env.ncols)\n",
    "        return x, y\n",
    "        \n",
    "    def _set_valid_start(self, x, y):\n",
    "        if self.env.is_end(x,y) or self.env.is_enemy(x,y):\n",
    "            return self._get_random_valid_pos()\n",
    "        else:\n",
    "            return x, y\n",
    "        \n",
    "    def _check_valid_start(self, x, y):\n",
    "        return not (self.env.is_end(x,y) or self.env.is_enemy(x,y))\n",
    "    \n",
    "    def __build_model__(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(24, input_dim=self.nstates, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(len(self.actions), activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=optimizers.Adam(lr=self.qsettings.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def set_starting_pos(self, *args):\n",
    "        if len(args) == 1:\n",
    "            # (x,y) as tuple\n",
    "            self.startx, self.starty = args[0]\n",
    "            if not self._check_valid_start(args[0][0], args[0][1]):\n",
    "                raise ValueError(\"Agent starting position is invalid\")\n",
    "        elif len(args) == 2:\n",
    "            # x,y as split params\n",
    "            if not self._check_valid_start(args[0], args[1]):\n",
    "                raise ValueError(\"Agent starting position is invalid\")\n",
    "            self.startx = args[0]\n",
    "            self.starty = args[1]\n",
    "        else:\n",
    "            raise ValueError(\"[SetStartingPos] Invalid number of arguments passed\")\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def get_next_action(self, x, y):\n",
    "        \"\"\"Next action is decided based on the epsilon-greedy algorithm.\n",
    "        If a random number is smaller than a value epsilon, the max value from the queue table is selected.\n",
    "        Otherwise, if the random number is larger, pick a random action\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.qsettings.epsilon:\n",
    "            if self.useDNN:\n",
    "                newstate = self.build_state(x, y)\n",
    "                act_values = self.model.predict(newstate) # Main difference between vanilla q-learning and DQL\n",
    "                result = np.argmax(act_values[0])\n",
    "            else:\n",
    "                result = np.argmax(self.qtable[x, y])\n",
    "            return result # returns action\n",
    "        else: #choose a random action\n",
    "            return np.random.randint(len(self.actions))\n",
    "    \n",
    "    def get_next_location(self,current_row_index, current_column_index, action_index: int):\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.env.ncols - 1:\n",
    "            new_column_index += 1\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.env.nrows - 1:\n",
    "            new_row_index += 1\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        return new_row_index, new_column_index\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = self.startx\n",
    "        self.y = self.starty\n",
    "        \n",
    "    def build_state(self, *args):\n",
    "        if len(args)==1:\n",
    "            # state\n",
    "            state = args[0]\n",
    "            return np.array([state,]) \n",
    "        elif len(args)==2:\n",
    "            # x, y\n",
    "            x = args[0]\n",
    "            y = args[1]\n",
    "            return np.array([[x, y],])  \n",
    "        else:\n",
    "            raise ValueError(\"[BuildState] Invalid number of arguments passed\")\n",
    "    \n",
    "    def step(self, actionindex):\n",
    "        # Take the step and calculate new state\n",
    "        self.x, self.y = self.get_next_location(self.x, self.y, actionindex)\n",
    "        newstate = self.build_state(self.x, self.y)\n",
    "        # Calculate reward\n",
    "        reward = self.env.rewards[self.x][self.y]\n",
    "        done = self.env.is_end(self.x, self.y) or self.env.is_enemy(self.x, self.y)\n",
    "        return newstate, reward, done\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.qsettings.discount * np.amax(t)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.qsettings.epsilon > self.qsettings.epsilon_min:\n",
    "            self.qsettings.epsilon *= self.qsettings.epsilon_decay\n",
    "    \n",
    "    def trainDNN(self, epochs: int, epoch_max_time: int, batch_size: int, randomizepos: bool = True, verbose: bool = False):\n",
    "        self.runinfo = RunInfo(epochs, epoch_max_time)\n",
    "        self.runinfo.start_timer()\n",
    "        done = False\n",
    "        for epoch in range(epochs):\n",
    "            if randomizepos:\n",
    "                self.startx, self.starty = self._get_random_valid_pos()\n",
    "            self.reset()\n",
    "            print(\"====> \",self.x, \" - \",self.y)\n",
    "            state = self.build_state(self.x, self.y)\n",
    "            time = 0\n",
    "            for time in range(epoch_max_time):\n",
    "                print(f\"Running time {time+1}/{epoch_max_time} for epoch {epoch+1}/{epochs}\", end='\\r', flush=True)\n",
    "                ### Choose an action using the epsilon-greedy algorithm\n",
    "                actionindex = self.get_next_action(self.x, self.y)\n",
    "                newstate, reward, done = self.step(actionindex)\n",
    "                # Store transition ( s,a,r,s',done )\n",
    "                self.memorize(state, actionindex, reward, newstate, done)\n",
    "                state = newstate\n",
    "                if done:\n",
    "                    self.update_target_model()\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                          .format(epoch+1, epochs, time+1, self.qsettings.epsilon))\n",
    "                    break\n",
    "                else:\n",
    "                    if len(self.memory) > batch_size: # If enough experiences in memory -> replay\n",
    "                        self.replay(batch_size)\n",
    "                self.runinfo.add_score(epoch, time, self.qsettings.epsilon)\n",
    "        self.runinfo.end_timer()\n",
    "        traintime = self.runinfo.get_timer()\n",
    "        self.trained = True\n",
    "        print(f\"Train completed in {traintime} seconds\")\n",
    "        \n",
    "    def train(self, epochs: int):\n",
    "        for epoch in range(epochs):\n",
    "            self.reset()\n",
    "            x = self.startx\n",
    "            y = self.starty\n",
    "            while (not self.env.is_end(self.x, self.y)) and (not self.env.is_enemy(self.x, self.y)):\n",
    "                actionindex = self.get_next_action(self.x, self.y)\n",
    "                oldrow, oldcol = self.x, self.y\n",
    "                self.x, self.y = self.get_next_location(self.x, self.y, actionindex)\n",
    "                \n",
    "                reward = self.env.rewards[self.x][self.y]\n",
    "                old_q = self.qtable[oldrow, oldcol, actionindex]\n",
    "                temporaldiff = reward + (self.qsettings.discount * np.max(self.qtable[self.x, self.y])) - old_q\n",
    "                new_q = old_q + (self.qsettings.learning_rate * temporaldiff)\n",
    "                self.qtable[oldrow, oldcol, actionindex] = new_q\n",
    "        self.trained = True\n",
    "    \n",
    "    def run(self, startingpos: tuple[int], maxiterations: int = 10000, verbose: bool = False):\n",
    "        self.set_starting_pos(startingpos)\n",
    "        if self.env.is_enemy(self.startx, self.starty) or self.env.is_end(self.startx, self.starty):\n",
    "            return []\n",
    "        else:\n",
    "            self.reset()\n",
    "            shortestpath = []\n",
    "            shortestpath.append((self.startx, self.starty))\n",
    "            #i: int = 0\n",
    "            while not (self.env.is_end(self.x, self.y) or self.env.is_enemy(self.x, self.y)):\n",
    "                actionindex = self.get_next_action(self.x, self.y)\n",
    "                self.x, self.y = self.get_next_location(self.x, self.y, actionindex)\n",
    "                shortestpath.append((self.x, self.y))\n",
    "                print(f\"({self.x},{self.y})\", end='\\r',flush=True)\n",
    "                #i+=1\n",
    "                #if (i >= maxiterations):\n",
    "                #    print(f\"Reached maximum iterations {maxiterations}. Current position: ({self.x},{self.y})\")\n",
    "                #    return []\n",
    "            self.ran = True\n",
    "            return shortestpath\n",
    "    \n",
    "\n",
    "    \n",
    "class Enemy(Character):\n",
    "    def __init__(self, x, y, radius = 3) -> None:\n",
    "        super().__init__(x,y)\n",
    "        self.radius = radius\n",
    "        \n",
    "    def inrange(self, x,y):\n",
    "        return Math.distance(x,self.x, y, self.y) <= self.radius\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9556fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self, nrows: int, ncols: int, starting_location: tuple, end_location: tuple, nenemies: int, ):\n",
    "        self.nrows = nrows\n",
    "        self.ncols = ncols\n",
    "        self.mapsize = (nrows, ncols)\n",
    "        self.end_location = end_location\n",
    "        self.endrow, self.endcol = self.end_location\n",
    "        self.nenemies = nenemies\n",
    "        \n",
    "        \n",
    "        self.enemies = []\n",
    "        self.rewards = np.full((self.nrows, self.ncols), -1.)\n",
    "        \n",
    "        self.basereward = -1\n",
    "        self.enemyreward = -300\n",
    "        self.endreward = 300\n",
    "        \n",
    "        self.__build_enemies__()\n",
    "        self.__build_rewards__()\n",
    "    \n",
    "        \n",
    "    def __build_enemies__(self):\n",
    "        if len(self.enemies) <= 0:\n",
    "            while len(self.enemies) < self.nenemies:\n",
    "                randx = np.random.randint(self.nrows)\n",
    "                randy = np.random.randint(self.ncols)\n",
    "                if (randx, randy) not in self.enemies:\n",
    "                    if (randx,randy) != self.end_location:\n",
    "                        self.enemies.append((randx, randy))\n",
    "                        \n",
    "    def __build_rewards__(self):\n",
    "        for enemy in self.enemies:\n",
    "            ob_x, ob_y = enemy\n",
    "            self.rewards[ob_x][ob_y] = self.enemyreward\n",
    "        self.rewards[self.end_location[0]][self.end_location[1]] = self.endreward\n",
    "    \n",
    "    \n",
    "    def is_enemy(self, x, y):\n",
    "        return self.rewards[x][x] == self.enemyreward\n",
    "    def is_end(self, x, y):\n",
    "        return self.rewards[x][y] == self.endreward\n",
    "    \n",
    "    def showmap(self):\n",
    "        if len(self.enemies) > 0:\n",
    "            fig, ax = plt.subplots(figsize=(9,9))\n",
    "            ax.set_title(\"Reward map\")\n",
    "            ax.set_xlabel(\"col\")\n",
    "            ax.set_ylabel(\"row\")\n",
    "            rewardmap = self.rewards\n",
    "            ax.imshow(rewardmap)\n",
    "            plt.show()      \n",
    "    \n",
    "    def showpath(self, startpos: tuple, shortestpath: list):\n",
    "        if len(shortestpath) > 0:\n",
    "            newmap = self.rewards\n",
    "            newmap[startpos[0]][startpos[1]] = -50\n",
    "            for pos in shortestpath:\n",
    "                r, c = pos\n",
    "                newmap[r][c] = 100\n",
    "            fig, ax = plt.subplots(figsize=(9,9))\n",
    "            ax.set_title(\"Reward map\")\n",
    "            ax.set_xlabel(\"col\")\n",
    "            ax.set_ylabel(\"row\")\n",
    "            ax.imshow(newmap)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f6855dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Environment(\n",
    "    30,30,\n",
    "    (1,1),\n",
    "    (28,28),\n",
    "    100\n",
    ")\n",
    "\n",
    "agentqsettings = QSettings(epsilon=0.9, discount=0.95, learning_rate=0.01)\n",
    "a = Agent(e, 1, 1, agentqsettings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19552e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "e.showmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eb941ca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-fcc6ed1860a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-99-bc09ddc2b6bf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_enemy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mactionindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                 \u001b[0moldrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moldcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactionindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-bc09ddc2b6bf>\u001b[0m in \u001b[0;36mget_next_action\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0museDNN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mnewstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mact_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Main difference between vanilla q-learning and DQL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1717\u001b[0m             steps=data_handler.inferred_steps)\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    346\u001b[0m             ' Always start with this line.')\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2753\u001b[0m         trackable=self, value=value, name=name)\n\u001b[1;32m   2754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2755\u001b[0;31m     \u001b[0mreference_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj_reference_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2756\u001b[0m     \u001b[0mreference_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_obj_reference_counts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2670\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_obj_reference_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m     \u001b[0;34m\"\"\"A dictionary counting the number of attributes referencing an object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m     self._maybe_create_attribute('_obj_reference_counts_dict',\n\u001b[0m\u001b[1;32m   2673\u001b[0m                                  object_identity.ObjectIdentityDictionary())\n\u001b[1;32m   2674\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj_reference_counts_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_create_attribute\u001b[0;34m(self, name, default_value)\u001b[0m\n\u001b[1;32m   2688\u001b[0m       \u001b[0mdefault_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m     \"\"\"\n\u001b[0;32m-> 2690\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a.train(5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58535b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "startpos = (1,1)\n",
    "shortestpath = a.run(startpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shortestpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.showpath(startpos, shortestpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a5c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(shortestpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb8951e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
