{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862f27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "\n",
    "# IPython\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#ML\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9556fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "\n",
    "@dataclass\n",
    "class QSettings():\n",
    "    epsilon: float\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.99\n",
    "    discount: float\n",
    "    learning_rate: float\n",
    "        \n",
    "\n",
    "class Environment():\n",
    "    def __init__(self, nrows: int, ncols: int, starting_location: tuple, end_location: tuple, nobstacles: int, qsettings: QSettings):\n",
    "        self.nrows = nrows\n",
    "        self.ncols = ncols\n",
    "        self.mapsize = (nrows, ncols)\n",
    "        self.starting_location = starting_location\n",
    "        self.startingrow, self.startingcol = self.starting_location\n",
    "        self.end_location = end_location\n",
    "        self.endrow, self.endcol = self.end_location\n",
    "        self.nobstacles = nobstacles\n",
    "        self.qsettings = qsettings\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.nstates = 2 # vertical movement (rows), horizontal movement (cols)\n",
    "        \n",
    "        self.currentrow, self.currentcol = self.starting_location\n",
    "        \n",
    "        self.obstacles = []\n",
    "        self.rewards = np.full((self.nrows, self.ncols), -1.)\n",
    "        self.qtable = np.zeros((self.nrows, self.ncols, len(self.actions))) # nrows x ncols x nactions\n",
    "        \n",
    "        self.basereward = -1\n",
    "        self.enemyreward = -300\n",
    "        self.endreward = 300\n",
    "        \n",
    "        self.__build_obstacles__()\n",
    "        self.__build_rewards__()\n",
    "        \n",
    "        self.memory = deque(maxlen=3000)\n",
    "        self.model = self.__build_model__()\n",
    "        self.target_model = self.__build_model__()\n",
    "        self.update_target_model()\n",
    "        \n",
    "        self.initialized = True\n",
    "        self.trained = False\n",
    "        self.ran = False\n",
    "    \n",
    "    def __build_model__(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(24, input_dim=self.nstates, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(len(self.actions), activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=optimizers.Adam(lr=self.qsettings.learning_rate))\n",
    "        return model\n",
    "        \n",
    "    def __build_obstacles__(self):\n",
    "        if len(self.obstacles) <= 0:\n",
    "            while len(self.obstacles) < self.nobstacles:\n",
    "                randrow = np.random.randint(self.nrows)\n",
    "                randcol = np.random.randint(self.ncols)\n",
    "                if (randrow, randcol) not in self.obstacles:\n",
    "                    if (randrow,randcol) != self.starting_location and (randrow,randcol) != self.end_location:\n",
    "                        self.obstacles.append((randrow, randcol))\n",
    "                        \n",
    "    def __build_rewards__(self):\n",
    "        for obstacle in self.obstacles:\n",
    "            ob_row, ob_col = obstacle\n",
    "            self.rewards[ob_row][ob_col] = self.enemyreward\n",
    "        self.rewards[self.end_location[0]][self.end_location[1]] = self.endreward\n",
    "    \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    \n",
    "    def is_enemy(self, row, col):\n",
    "        return self.rewards[row][col] == self.enemyreward\n",
    "    def is_end(self, row, col):\n",
    "        return self.rewards[row][col] == self.endreward\n",
    "    \n",
    "    def get_next_action(self, current_row_index, current_column_index):\n",
    "        \"\"\"Next action is decided based on the epsilon-greedy algorithm.\n",
    "        If a random number is smaller than a value epsilon, the max value from the queue table is selected.\n",
    "        Otherwise, if the random number is larger, pick a random action\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.qsettings.epsilon:\n",
    "            x = self.build_state(self.currentrow, self.currentcol)\n",
    "            act_values = self.model.predict(x) # Main difference between vanilla q-learning and DQL\n",
    "            return np.argmax(act_values[0])  # returns action\n",
    "        else: #choose a random action\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "    def get_next_location(self,current_row_index, current_column_index, action_index: int):\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.ncols - 1:\n",
    "            new_column_index += 1\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.nrows - 1:\n",
    "            new_row_index += 1\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        return new_row_index, new_column_index\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                # a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.qsettings.discount * np.amax(t)\n",
    "                # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.qsettings.epsilon > self.qsettings.epsilon_min:\n",
    "            self.qsettings.epsilon *= self.qsettings.epsilon_decay\n",
    "    \n",
    "    def showmap(self):\n",
    "        if len(self.obstacles) > 0:\n",
    "            fig, ax = plt.subplots(figsize=(9,9))\n",
    "            ax.set_title(\"Reward map\")\n",
    "            ax.set_xlabel(\"col\")\n",
    "            ax.set_ylabel(\"row\")\n",
    "            rewardmap = self.rewards\n",
    "            rewardmap[self.startingrow][self.startingcol] = -50\n",
    "            ax.imshow(rewardmap)\n",
    "            plt.show()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.currentrow = self.startingrow\n",
    "        self.currentcol = self.startingcol\n",
    "        \n",
    "    def build_state(self, currenrow, currentcol):\n",
    "        return np.array([[self.currentrow, self.currentcol],])\n",
    "            \n",
    "    def train(self, epochs: int, epoch_max_time: int, batch_size: int, verbose: bool = False):\n",
    "        done = False\n",
    "        for epoch in range(epochs):\n",
    "            self.reset()\n",
    "            row, col = self.starting_location\n",
    "            state = self.build_state(self.currentrow, self.currentcol)\n",
    "            for time in range(epoch_max_time):\n",
    "                print(f\"Running time {time}/{epoch_max_time} for epoch {epoch}/{epochs}\", end='\\r', flush=True)\n",
    "                ### Choose an action using the epsilon-greedy algorithm\n",
    "                actionindex = self.get_next_action(self.currentrow, self.currentcol)\n",
    "                # Take the step and calculate new state\n",
    "                self.currentrow, self.currentcol = self.get_next_location(self.currentrow, self.currentcol, actionindex)\n",
    "                newstate = self.build_state(self.currentrow, self.currentcol)\n",
    "                # Calculate reward\n",
    "                reward = self.rewards[self.currentrow][self.currentcol]\n",
    "                done = self.is_end(self.currentrow, self.currentcol)\n",
    "                # Store transition ( s,a,r,s',done )\n",
    "                self.memorize(state, actionindex, reward, newstate, done)\n",
    "                state = newstate\n",
    "                if done:\n",
    "                    agent.update_target_model()\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                          .format(e, EPISODES, time, agent.epsilon))\n",
    "                    break\n",
    "                else:\n",
    "                    if len(self.memory) > batch_size: # If enough experiences in memory -> replay\n",
    "                        self.replay(batch_size)\n",
    "        self.trained = True\n",
    "        \n",
    "    \n",
    "    def run(self, maxiterations: int = 10000, verbose: bool = False):\n",
    "        if self.is_enemy(self.startingrow, self.startingcol) or self.is_end(self.startingrow, self.startingcol):\n",
    "            return []\n",
    "        else:\n",
    "            self.reset()\n",
    "            shortestpath = []\n",
    "            shortestpath.append(self.starting_location)\n",
    "            #i: int = 0\n",
    "            while (not self.is_end(self.currentrow, self.currentcol)) and (not self.is_enemy(self.currentrow, self.currentcol)):\n",
    "                actionindex = self.get_next_action(self.currentrow, self.currentcol)\n",
    "                self.currentrow, self.currentcol = self.get_next_location(self.currentrow, self.currentcol, actionindex)\n",
    "                shortestpath.append((self.currentrow, self.currentcol))\n",
    "                #i+=1\n",
    "                #if (i >= maxiterations):\n",
    "                #    print(f\"Reached maximum iterations {maxiterations}. Current position: ({self.currentrow},{self.currentcol})\")\n",
    "                #    return []\n",
    "            self.ran = True\n",
    "            return shortestpath\n",
    "    \n",
    "    def showpath(self, shortestpath: list):\n",
    "        if self.run and len(shortestpath) > 0:\n",
    "            newmap = self.rewards\n",
    "            newmap[self.startingrow][self.startingcol] = -50\n",
    "            for pos in shortestpath:\n",
    "                r, c = pos\n",
    "                newmap[r][c] = 100\n",
    "            fig, ax = plt.subplots(figsize=(9,9))\n",
    "            ax.set_title(\"Reward map\")\n",
    "            ax.set_xlabel(\"col\")\n",
    "            ax.set_ylabel(\"row\")\n",
    "            ax.imshow(newmap)\n",
    "            plt.show()\n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6855dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "e = Environment(\n",
    "    30,30,\n",
    "    (1,1),\n",
    "    (28,28),\n",
    "    100,\n",
    "    QSettings(epsilon=0.9, discount=0.95, learning_rate=0.9)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19552e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "e.showmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb941ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time 44/50 for epoch 3/5\r"
     ]
    }
   ],
   "source": [
    "e.train(5, 50, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a73e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shortestpath = e.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#e.showpath(shortestpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a5c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(shortestpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
