{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a736c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "\n",
    "# ML\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3d1e2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QSettings():\n",
    "    epsilon: float = 1.0\n",
    "    default_epsilon: float = epsilon\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.99\n",
    "    discount: float = 0.9\n",
    "    learning_rate: float = 0.001\n",
    "        \n",
    "    def reset(self):\n",
    "        self.epsilon = self.default_epsilon\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b4dcdf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.48462760307470476, 4.457829706677577)\n"
     ]
    }
   ],
   "source": [
    "testt = [ \n",
    "    np.array([0, 1], dtype=np.float32),  # radial movement\n",
    "    np.array([0, 2*np.pi], dtype=np.float32), # Anglular movement\n",
    "]\n",
    "neww = (np.random.uniform(testt[0][0], testt[0][1]), np.random.uniform(testt[1][0], testt[1][1]))\n",
    "print(neww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8b6fc11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Math:\n",
    "    @staticmethod\n",
    "    def cartesian_to_polar(x,y):\n",
    "        return (np.sqrt(x*x + y*y), np.arctan2(y,x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def polar_to_cartesian(r, theta):\n",
    "        return (r*np.cos(theta), r*np.sin(theta))\n",
    "    \n",
    "    @staticmethod\n",
    "    def distance(x1, x2, y1, y2):\n",
    "        return np.sqrt( np.power( (x2 - x1) , 2) + np.power( (y2 - y1 ) , 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "319ce0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character:\n",
    "    def __init__(self, x, y, health: float, attdmg: float):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.startingx = x\n",
    "        self.startingy = y\n",
    "        self.health = health\n",
    "        self.attdmg = attdmg\n",
    "\n",
    "# Move qsettings to agent (?)\n",
    "        \n",
    "class Agent(Character):\n",
    "    def __init__(self, env, x, y):\n",
    "        super().__init__(x,y, 200, 10)\n",
    "        self.env = env\n",
    "        self.memory = deque(maxlen=3000)\n",
    "        \n",
    "        self.actions = [ \n",
    "            np.array([0, 1], dtype=np.float32),  # radial movement\n",
    "            np.array([0, 2*np.pi], dtype=np.float32), # Anglular movement\n",
    "        ]\n",
    "        self.action_size = len(self.actions)\n",
    "        \n",
    "        self.state_size = 2 # (x, y)\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds state-action DNN model\n",
    "        \"\"\"\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(24, input_dim = self.state_size, activation='relu')) # state size is 2 (agent x, agent y)\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=optimizers.Adam(lr=self.env.qsettings.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    \n",
    "    # model must hold polar coordinates because it's relative to the action\n",
    "    def get_next_action(self, state):\n",
    "        if np.random.rand() <= self.env.qsettings.epsilon:\n",
    "            return (np.random.uniform(testt[0][0], testt[0][1]), np.random.uniform(testt[1][0], testt[1][1]))\n",
    "        act_values = self.model.predict(state)\n",
    "        return act_values\n",
    "    \n",
    "    def move(self, x, y, action):\n",
    "        assert isinstance(action, tuple)\n",
    "        assert len(action)==2\n",
    "        r = action[0]\n",
    "        theta = action[1]\n",
    "        newx, newy = Math.polar_to_cartesian(r, theta)\n",
    "        return newx, newy\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        self.x, self.y = self.move(self.x, self.y, action)\n",
    "        if len(self.env.nearby_enemies(self.x, self.y)) > 0:\n",
    "            reward = -20\n",
    "        elif self.env.is_end(self.x, self.y):\n",
    "            reward = 100\n",
    "        else:\n",
    "            reward = -1\n",
    "        state = (self.x, self.y)\n",
    "        done = self.env.is_end(self.x, self.y)\n",
    "        return (state, reward, done)\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "    def replay(self, batch_size: int):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                trarget[0][actoin] = reward + self.qsettings.discount * np.amax(t)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.qsettings.epsilon > self.qsettings.epsilon_min:\n",
    "            self.qsettings.epsilon *= self.qsettings.epsilon_decay\n",
    "            \n",
    "    def build_state(self, x, y):\n",
    "        return np.array([x,y]), {}\n",
    "            \n",
    "    def reset(self):\n",
    "        self.env.qsettings.reset()\n",
    "        # Reset state\n",
    "        self.x = self.startingx\n",
    "        self.y = self.startingy\n",
    "        return self.build_state(self.x, self.y)\n",
    "    \n",
    "    def train(self, epochs: int, max_time_limit: int, batch_size: int):\n",
    "        done = False\n",
    "        for epoch in range(epochs):\n",
    "            state = self.reset()\n",
    "            for time in range(max_time_limit):\n",
    "                action = self.get_next_action(state)\n",
    "                next_state, reward, done = self.step(action)\n",
    "                self.memorize(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    self.update_target_model()\n",
    "                    print(\"episode: {}/{}, score: {}/{}, e: {:.2}\"\n",
    "                      .format(epoch, epochs-1, time, max_time_limit-1, self.env.qsettings.epsilon))\n",
    "                else:\n",
    "                    self.replay(batch_size)\n",
    "                \n",
    "        \n",
    "            \n",
    "        \n",
    "class Enemy(Character):\n",
    "    def __init__(self, x,y, radius=10):\n",
    "        super().__init__(x,y, 50, 7)\n",
    "        self.radius = radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d4d315a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self, mapsize: tuple = (30,30), timeout = 500, nenemies = 50, endpos: tuple = (0,0) , qsettings = QSettings()):\n",
    "        self.mapsize = mapsize\n",
    "        self.qsettings = qsettings\n",
    "        self.enemies = []\n",
    "        self.endpos = endpos\n",
    "        self._build_enemies(nenemies)\n",
    "    \n",
    "    def _build_enemies(self, nenemies):\n",
    "        x = np.random.uniform(0, self.mapsize[0], size=nenemies)\n",
    "        y = np.random.uniform(0, self.mapsize[1], size=nenemies)\n",
    "        for i in range(len(x)):\n",
    "            self.enemies.append(Enemy(x[i], y[i]))\n",
    "            \n",
    "    def nearby_enemies(self, x, y):\n",
    "        inrange_enemies = []\n",
    "        for enemy in self.enemies:\n",
    "            if Math.distance(x, enemy.x, y, enemy.y) < enemy.radius:\n",
    "                inrange_enemies.append(enemy)\n",
    "        return inrange_enemies \n",
    "    \n",
    "    def is_end(self, x, y):\n",
    "        if x == self.endpos[0] and y == self.endpos[1]:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "34962f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Environment(nenemies=55)\n",
    "a = Agent(e,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "be7f9538",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-69e9e9b7065b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-161-92a137f036ba>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, max_time_limit, batch_size)\u001b[0m\n\u001b[1;32m    116\u001b[0m                       .format(epoch, epochs-1, time, max_time_limit-1, self.env.qsettings.epsilon))\n\u001b[1;32m    117\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-92a137f036ba>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "a.train(100, 64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d60f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
