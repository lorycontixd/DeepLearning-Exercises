{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6b6141",
   "metadata": {},
   "source": [
    "https://pythonprogramming.net/q-learning-analysis-reinforcement-learning-python-tutorial/\n",
    "\n",
    "# Vanilla Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb809ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import IntEnum\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete, MultiDiscrete\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a9c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QSettings():\n",
    "    nepisodes: int\n",
    "    epsilon: float\n",
    "    discount: float\n",
    "    learning_rate: float\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.epsilon_start_decaying = 1\n",
    "        self.epsilon_end_decaying = self.nepisodes\n",
    "        self.epsilon_decay_value = self.epsilon/(self.epsilon_end_decaying - self.epsilon_start_decaying)\n",
    "\n",
    "''' Helper class to enumerate actions in the grid levels '''\n",
    "class Actions(IntEnum):  \n",
    "    STAY  = 0    \n",
    "    UP = 1\n",
    "    LEFT  = 2\n",
    "    DOWN = 3\n",
    "    RIGHT  = 4\n",
    "\n",
    "    # get the enum name without the class\n",
    "    def __str__(self): return self.name   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e7c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(Env):\n",
    "    def __init__(self, size: int, starting_location: tuple, end_location: tuple, nobstacles: int, qsettings: QSettings):\n",
    "        self.__verify_input(size, starting_location, end_location, nobstacles)\n",
    "        self.size = size\n",
    "        self.nrows = size\n",
    "        self.ncols = size\n",
    "        self.mapsize = (size, size)\n",
    "        self.starting_location = starting_location\n",
    "        self.default_location = self.starting_location\n",
    "        self.startingrow, self.startingcol = self.starting_location\n",
    "        self.end_location = end_location\n",
    "        self.endrow, self.endcol = self.end_location\n",
    "        self.nobstacles = nobstacles\n",
    "        self.qsettings = qsettings\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.action_space = Discrete(len(self.actions))\n",
    "        self.observation_space = MultiDiscrete([ self.nrows, self.ncols ])\n",
    "        \n",
    "        self.x, self.y = self.starting_location\n",
    "        \n",
    "        self.obstacles = []\n",
    "        self.checkpoinst = []\n",
    "        self.rewards = np.full((self.nrows, self.ncols), -1)\n",
    "        #self.qtable = np.random.uniform(low=-1, high=1, size=[self.size, self.size, self.action_space.n])\n",
    "        self.basereward = -1\n",
    "        self.enemyreward = -300\n",
    "        self.endreward = 300\n",
    "        \n",
    "        self.qtable = np.full(shape= [self.size, self.size, self.action_space.n], fill_value=0)\n",
    "        \n",
    "        self.__build_obstacles__()\n",
    "        self.__build_rewards__()\n",
    "        \n",
    "        self.initialized = True\n",
    "        #self.__DISCRETE_OS_SIZE = [self.size] * len(self.observation_space.high)\n",
    "        #self.__DISCRETE_OS_WIN_SIZE = (self.observation_space.high - self.observation_space.low)/self.__DISCRETE_OS_SIZE\n",
    "        \n",
    "    def __verify_input(self, size: int, starting_location: tuple, end_location: tuple, nenemies: int):\n",
    "        if (size <= 0):\n",
    "            raise ValueError(\"Cannot set negative or null size\")\n",
    "        if (len(starting_location) != 2):\n",
    "            raise ValueError(f\"Starting location must have 2 elements, received {len(starting_location)}. Starting x and starting y.\")\n",
    "        if (starting_location[0] >= size or starting_location[1] >= size):\n",
    "            raise ValueError(f\"Starting location is outside of map bounds. Received location of {starting_location}, but map is a square with {size} tiles.\")\n",
    "        if (len(end_location) != 2):\n",
    "            raise ValueError(f\"End location must have 2 elements, received {len(end_location)}. Starting x and starting y.\")\n",
    "        if (end_location[0] >= size or end_location[1] >= size):\n",
    "            raise ValueError(f\"End location is outside of map bounds. Received location of {end_location}, but map is a square with {size} tiles.\")\n",
    "        if (end_location == starting_location):\n",
    "            raise ValueError(\"Cannot set ending location on starting location.\")\n",
    "        if (nenemies > size*size - 2):\n",
    "            raise ValueError(f\"Too many enemies set. Max value allowed is {size*size - 2}, but received {nenemies}\")\n",
    "        if (nenemies <= 0):\n",
    "            raise ValueError(\"Cannot set negative or no enemies\")\n",
    "            \n",
    "        \n",
    "    def __build_obstacles__(self):\n",
    "        if len(self.obstacles) <= 0:\n",
    "            while len(self.obstacles) < self.nobstacles:\n",
    "                randrow = np.random.randint(self.nrows)\n",
    "                randcol = np.random.randint(self.ncols)\n",
    "                if (randrow, randcol) not in self.obstacles:\n",
    "                    if (randrow,randcol) != self.starting_location and (randrow,randcol) != self.end_location:\n",
    "                        self.obstacles.append((randrow, randcol))\n",
    "    def __build_checkpoints(self):\n",
    "        if len(self.checkpoints <= 0):\n",
    "            while len(self.checkpoints) < 5:\n",
    "                randrow = np.random.randint(self.nrows)\n",
    "                randcol = np.random.randint(self.ncols)\n",
    "                if (randrow, randcol) not in self.checkpoints:\n",
    "                        if (randrow,randcol) != self.starting_location and (randrow,randcol) != self.end_location and (randrow, randcol) not in self.obstacles:\n",
    "                            self.checkpoints.append((randrow, randcol))\n",
    "                        \n",
    "    def __build_rewards__(self):\n",
    "        for obstacle in self.obstacles:\n",
    "            ob_row, ob_col = obstacle\n",
    "            self.rewards[ob_row][ob_col] = self.enemyreward\n",
    "        self.rewards[self.end_location[0]][self.end_location[1]] = self.endreward\n",
    "    \n",
    "    \n",
    "    def is_enemy(self, row, col):\n",
    "        return self.rewards[row][col] == self.enemyreward\n",
    "    def is_end(self, row, col):\n",
    "        return self.rewards[row][col] == self.endreward\n",
    "    \n",
    "    \n",
    "    def get_next_action(self, current_row_index, current_column_index):\n",
    "        \"\"\"Next action is decided based on the epsilon-greedy algorithm.\n",
    "        If a random number is smaller than a value epsilon, the max value from the queue table is selected.\n",
    "        Otherwise, if the random number is larger, pick a random action\n",
    "        \"\"\"\n",
    "        value = 0\n",
    "        if np.random.random() >= self.qsettings.epsilon:\n",
    "            value = np.argmax(self.qtable[current_row_index, current_column_index])\n",
    "        else: #choose a random action\n",
    "            value = np.random.randint(len(self.actions))\n",
    "        return value\n",
    "        \n",
    "    def get_next_location(self,current_row_index, current_column_index, action_index: int):\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        try:\n",
    "            if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "                new_row_index -= 1\n",
    "            elif self.actions[action_index] == 'right' and current_column_index < self.ncols - 1:\n",
    "                new_column_index += 1\n",
    "            elif self.actions[action_index] == 'down' and current_row_index < self.nrows - 1:\n",
    "                new_row_index += 1\n",
    "            elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "                new_column_index -= 1\n",
    "        except:\n",
    "            print(\"!!\" + str(action_index))\n",
    "        return new_row_index, new_column_index\n",
    "    \n",
    "    def build_state(self, *args):\n",
    "        if len(args)==1:\n",
    "            # state\n",
    "            state = args[0]\n",
    "            return np.array([state,]) \n",
    "        elif len(args)==2:\n",
    "            # x, y\n",
    "            x = args[0]\n",
    "            y = args[1]\n",
    "            return np.array([[x, y],])  \n",
    "        else:\n",
    "            raise ValueError(\"[BuildState] Invalid number of arguments passed\")\n",
    "    \n",
    "    def step(self, actionindex: int):\n",
    "        self.x, self.y = self.get_next_location(self.x, self.y, actionindex)\n",
    "        newstate = (self.x, self.y)\n",
    "        truncated = False\n",
    "        #Â Calculate reward\n",
    "        reward = self.rewards[self.x][self.y]\n",
    "        done = self.is_end(self.x, self.y) or self.is_enemy(self.x, self.y)\n",
    "        info = {}\n",
    "        return newstate, reward, done, truncated, info\n",
    "    \n",
    "    def showmap(self):\n",
    "        print(len(self.obstacles))\n",
    "        if len(self.obstacles) > 0:\n",
    "            fig, ax = plt.subplots(figsize=(9,9))\n",
    "            ax.set_title(\"Reward map\")\n",
    "            ax.set_xlabel(\"col\")\n",
    "            ax.set_ylabel(\"row\")\n",
    "            rewardmap = self.rewards\n",
    "            rewardmap[self.startingrow][self.startingcol] = -50\n",
    "            ax.imshow(rewardmap)\n",
    "            plt.show()\n",
    "    \n",
    "    def showpath(self, path: list):\n",
    "        if len(path) > 0:\n",
    "            plt.clf()\n",
    "            newmap = self.rewards\n",
    "            newmap[path[0][0]][path[0][1]] = -50\n",
    "            print(\"===> \",path)\n",
    "            for pos in path:\n",
    "                r, c = pos\n",
    "                newmap[r][c] = 100\n",
    "            fig2, ax2 = plt.subplots(figsize=(9,9))\n",
    "            ax2.set_title(\"Reward map\")\n",
    "            ax2.set_xlabel(\"col\")\n",
    "            ax2.set_ylabel(\"row\")\n",
    "            ax2.imshow(newmap)\n",
    "        plt.show()\n",
    "        \n",
    "    def reset(self, randomize: bool = False):\n",
    "        if randomize:\n",
    "            state = np.random.randint(low=0, high=self.size, size=(2,))\n",
    "        else:\n",
    "            state = self.default_location\n",
    "        self.x = state[0]\n",
    "        self.y = state[1]\n",
    "        return state\n",
    "    \n",
    "    #def reset(self):\n",
    "    #    self.x = self.x\n",
    "    #    self.y = self.y\n",
    "    #    return np.asarray((self.x, self.y))\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    # Properties\n",
    "    def get_epsilon(self):\n",
    "        return self.qsettings.epsilon\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "107f453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 100000\n",
    "SHOW_EVERY = 1000\n",
    "STATS_EVERY = 1000\n",
    "EPSILON = 0.998\n",
    "\n",
    "qsettings = QSettings(nepisodes = EPISODES, epsilon= EPSILON, discount= DISCOUNT, learning_rate= LEARNING_RATE)\n",
    "env = Environment(30, (2,2), (25,25), 100, qsettings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c7e469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.showmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55be6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stats\n",
    "ep_rewards = []\n",
    "aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': [], 'epsilon': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b13e3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:     0, average reward: -0.3, current epsilon: 1.00\n",
      "Episode:  1000, average reward: -374.4, current epsilon: 0.99\n",
      "Episode:  2000, average reward: -379.1, current epsilon: 0.98\n",
      "Episode:  3000, average reward: -378.9, current epsilon: 0.97\n",
      "Episode:  4000, average reward: -377.7, current epsilon: 0.96\n",
      "Episode:  5000, average reward: -383.0, current epsilon: 0.95\n",
      "Episode:  6000, average reward: -379.0, current epsilon: 0.94\n",
      "Episode:  7000, average reward: -380.2, current epsilon: 0.93\n",
      "Episode:  8000, average reward: -384.9, current epsilon: 0.92\n",
      "Episode:  9000, average reward: -382.4, current epsilon: 0.91\n",
      "Episode: 10000, average reward: -382.9, current epsilon: 0.90\n",
      "Episode: 11000, average reward: -381.3, current epsilon: 0.89\n",
      "Episode: 12000, average reward: -383.7, current epsilon: 0.88\n",
      "Episode: 13000, average reward: -387.4, current epsilon: 0.87\n",
      "Episode: 14000, average reward: -389.7, current epsilon: 0.86\n",
      "Episode: 15000, average reward: -386.9, current epsilon: 0.85\n",
      "Episode: 16000, average reward: -381.3, current epsilon: 0.84\n",
      "Episode: 17000, average reward: -388.8, current epsilon: 0.83\n",
      "Episode: 18000, average reward: -386.2, current epsilon: 0.82\n",
      "Episode: 19000, average reward: -392.0, current epsilon: 0.81\n",
      "Episode: 20000, average reward: -389.7, current epsilon: 0.80\n",
      "Episode: 21000, average reward: -390.2, current epsilon: 0.79\n",
      "Episode: 22000, average reward: -389.4, current epsilon: 0.78\n",
      "Episode: 23000, average reward: -387.6, current epsilon: 0.77\n",
      "Episode: 24000, average reward: -395.7, current epsilon: 0.76\n",
      "Episode: 25000, average reward: -397.2, current epsilon: 0.75\n",
      "Episode: 26000, average reward: -395.6, current epsilon: 0.74\n",
      "Episode: 27000, average reward: -395.7, current epsilon: 0.73\n",
      "Episode: 28000, average reward: -397.8, current epsilon: 0.72\n",
      "Episode: 29000, average reward: -404.0, current epsilon: 0.71\n",
      "Episode: 30000, average reward: -393.3, current epsilon: 0.70\n",
      "Episode: 31000, average reward: -397.0, current epsilon: 0.69\n",
      "Episode: 32000, average reward: -400.1, current epsilon: 0.68\n",
      "Episode: 33000, average reward: -400.6, current epsilon: 0.67\n",
      "Episode: 34000, average reward: -398.3, current epsilon: 0.66\n",
      "Episode: 35000, average reward: -403.3, current epsilon: 0.65\n",
      "Episode: 36000, average reward: -404.6, current epsilon: 0.64\n",
      "Episode: 37000, average reward: -411.4, current epsilon: 0.63\n",
      "Episode: 38000, average reward: -414.4, current epsilon: 0.62\n",
      "Episode: 39000, average reward: -405.0, current epsilon: 0.61\n",
      "Episode: 40000, average reward: -411.6, current epsilon: 0.60\n",
      "Episode: 41000, average reward: -407.9, current epsilon: 0.59\n",
      "Episode: 42000, average reward: -415.0, current epsilon: 0.58\n",
      "Episode: 43000, average reward: -414.8, current epsilon: 0.57\n",
      "Episode: 44000, average reward: -422.0, current epsilon: 0.56\n",
      "Episode: 45000, average reward: -416.6, current epsilon: 0.55\n",
      "Episode: 46000, average reward: -427.7, current epsilon: 0.54\n",
      "Episode: 47000, average reward: -424.9, current epsilon: 0.53\n",
      "Episode: 48000, average reward: -424.4, current epsilon: 0.52\n",
      "Episode: 49000, average reward: -425.2, current epsilon: 0.51\n",
      "Episode: 50000, average reward: -432.5, current epsilon: 0.50\n",
      "Episode: 51000, average reward: -442.4, current epsilon: 0.49\n",
      "Episode: 52000, average reward: -449.8, current epsilon: 0.48\n",
      "Episode: 53000, average reward: -439.3, current epsilon: 0.47\n",
      "Episode: 54000, average reward: -442.9, current epsilon: 0.46\n",
      "Episode: 55000, average reward: -455.3, current epsilon: 0.45\n",
      "Episode: 56000, average reward: -452.3, current epsilon: 0.44\n",
      "Episode: 57000, average reward: -462.4, current epsilon: 0.43\n",
      "Episode: 58000, average reward: -464.4, current epsilon: 0.42\n",
      "Episode: 59000, average reward: -463.1, current epsilon: 0.41\n",
      "Episode: 60000, average reward: -477.2, current epsilon: 0.40\n",
      "Episode: 61000, average reward: -480.7, current epsilon: 0.39\n",
      "Episode: 62000, average reward: -478.7, current epsilon: 0.38\n",
      "Episode: 63000, average reward: -497.2, current epsilon: 0.37\n",
      "Episode: 64000, average reward: -502.1, current epsilon: 0.36\n",
      "Episode: 65000, average reward: -507.3, current epsilon: 0.35\n",
      "Episode: 66000, average reward: -519.9, current epsilon: 0.34\n",
      "Episode: 67000, average reward: -529.2, current epsilon: 0.33\n",
      "Episode: 68000, average reward: -525.3, current epsilon: 0.32\n",
      "Episode: 69000, average reward: -547.9, current epsilon: 0.31\n",
      "Episode: 70000, average reward: -545.1, current epsilon: 0.30\n",
      "Episode: 71000, average reward: -560.5, current epsilon: 0.29\n",
      "Episode: 72000, average reward: -570.8, current epsilon: 0.28\n",
      "Episode: 73000, average reward: -590.6, current epsilon: 0.27\n",
      "Episode: 74000, average reward: -588.6, current epsilon: 0.26\n",
      "Episode: 75000, average reward: -595.7, current epsilon: 0.25\n",
      "Episode: 76000, average reward: -621.2, current epsilon: 0.24\n",
      "Episode: 77000, average reward: -631.6, current epsilon: 0.23\n",
      "Episode: 78000, average reward: -666.8, current epsilon: 0.22\n",
      "Episode: 79000, average reward: -689.2, current epsilon: 0.21\n",
      "Episode: 80000, average reward: -713.3, current epsilon: 0.20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-98d01a7b0ad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Update Q table with new Q value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "goodpaths = []\n",
    "for episode in range(EPISODES):\n",
    "    episode_reward = 0 # Reset episode reward\n",
    "    episode_steps = 0 # Reset episode steps\n",
    "    state = env.reset(False)\n",
    "    #if not episode %  STATS_EVERY: \n",
    "    #    print(f\"Starting episode at: {state}\")\n",
    "    done = False\n",
    "\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        render = True\n",
    "    else:\n",
    "        render = False\n",
    "    \n",
    "    path = []\n",
    "    while not done:\n",
    "        action = env.get_next_action(env.x, env.y)\n",
    "\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        #print(f\"===> {state} - {new_state}\")\n",
    "        episode_steps += 1\n",
    "        episode_reward += reward\n",
    "        path.append(state)\n",
    "\n",
    "        #if episode % SHOW_EVERY == 0:\n",
    "        #    env.render()\n",
    "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "        # If simulation did not end yet after last step - update Q table\n",
    "        \n",
    "        if not done:\n",
    "\n",
    "            # Maximum possible Q value in next step (for new state)\n",
    "            max_future_q = np.max(env.qtable[new_state[0],new_state[1]])\n",
    "\n",
    "            # Current Q value (for current state and performed action):\n",
    "            current_q = env.qtable[state[0],state[1],action]\n",
    "\n",
    "            # And here's our equation for a new Q value for current state and action\n",
    "            new_q = (1 - env.qsettings.learning_rate) * current_q + env.qsettings.learning_rate * (reward + env.qsettings.discount * max_future_q)\n",
    "            #oldq = env.qtable[state[0], state[1], action]\n",
    "            #temporaldiff = reward + (env.qsettings.discount * np.max(env.qtable[new_state[0], new_state[1]])) - oldq\n",
    "            #new_q = oldq + (env.qsettings.learning_rate * temporaldiff)\n",
    "            \n",
    "            # Update Q table with new Q value\n",
    "            env.qtable[state[0]][state[1]][action] = new_q\n",
    "\n",
    "\n",
    "        # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "        else:\n",
    "            if (env.is_end(new_state[0], new_state[1])):\n",
    "                env.qtable[state[0], state[1], action] = reward\n",
    "                goodpaths.append(path)\n",
    "            #print(f\"EPISODE DONE ==>  reached food: {new_state == env.end_location}, reward: {episode_reward}, steps: {episode_steps}\")\n",
    "            #if (new_state & np.array(env.end_location)).all():\n",
    "            #    env.qtable[state[0], state[1], action] = reward\n",
    "            #    #env.qtable[state[0], state[1], action] = 0\n",
    "            #    goodpaths.append(path)\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "    # Decaying is being done every episode if episode number is within decaying range\n",
    "    if env.qsettings.epsilon_end_decaying >= episode >= env.qsettings.epsilon_start_decaying:\n",
    "        env.qsettings.epsilon -= env.qsettings.epsilon_decay_value\n",
    "        \n",
    "        \n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % STATS_EVERY:\n",
    "        average_reward = sum(ep_rewards[-STATS_EVERY:])/STATS_EVERY\n",
    "        aggr_ep_rewards['ep'].append(episode)\n",
    "        aggr_ep_rewards['avg'].append(average_reward)\n",
    "        aggr_ep_rewards['max'].append(max(ep_rewards[-STATS_EVERY:]))\n",
    "        aggr_ep_rewards['min'].append(min(ep_rewards[-STATS_EVERY:]))\n",
    "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {env.get_epsilon():>1.2f}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68446e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60effc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['avg'], label=\"average rewards\")\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['max'], label=\"max rewards\")\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['min'], label=\"min rewards\")\n",
    "plt.legend(loc=4)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e0ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(goodpaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99777a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
